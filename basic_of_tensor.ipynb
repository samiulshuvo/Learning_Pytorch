{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.7725e-21,  4.5895e-41],\n",
      "        [-7.7053e-35,  3.0644e-41],\n",
      "        [ 0.0000e+00,  0.0000e+00]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "x=torch.empty(3,2)\n",
    "print(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5163, 0.6662, 0.3572],\n",
      "        [0.9209, 0.9972, 0.0017]])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "x_rand=torch.rand(2,3)\n",
    "print(x_rand)\n",
    "print(x_rand.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "x_dt=torch.ones(2,2,dtype=torch.float16)\n",
    "print(x_dt.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7399, 1.7881],\n",
      "        [0.7786, 1.3155]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.rand(2,2)\n",
    "y=torch.rand(2,2)\n",
    "print(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3756, 2.6697],\n",
       "        [1.4816, 1.8891]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(x)## _ means in place operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4621, 0.3302],\n",
       "        [0.4745, 0.3036]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.subtract(x,y)\n",
    "torch.mul(x,y)\n",
    "torch.div(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6952, 0.6693, 0.4393],\n",
      "        [0.3997, 0.0474, 0.9590],\n",
      "        [0.0713, 0.1092, 0.1300],\n",
      "        [0.0143, 0.2588, 0.5623],\n",
      "        [0.4019, 0.8602, 0.2614]])\n"
     ]
    }
   ],
   "source": [
    "x_slice=torch.rand(5,3)\n",
    "print(x_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6693, 0.0474, 0.1092, 0.2588, 0.8602])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_slice[:,1] ## slicing like numpy array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6952, 0.6693, 0.4393, 0.3997, 0.0474],\n",
       "        [0.9590, 0.0713, 0.1092, 0.1300, 0.0143],\n",
       "        [0.2588, 0.5623, 0.4019, 0.8602, 0.2614]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_slice.reshape(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6952, 0.6693, 0.4393, 0.3997, 0.0474, 0.9590, 0.0713, 0.1092, 0.1300,\n",
       "        0.0143, 0.2588, 0.5623, 0.4019, 0.8602, 0.2614])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_slice.reshape(15,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6952, 0.6693, 0.4393, 0.3997, 0.0474],\n",
       "        [0.9590, 0.0713, 0.1092, 0.1300, 0.0143],\n",
       "        [0.2588, 0.5623, 0.4019, 0.8602, 0.2614]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_slice.view(3,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_array=np.ones([3,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpytotorch=torch.tensor(numpy_array,dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1]], dtype=int32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(numpytotorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1]], dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpytotorch.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(numpy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bynoy can only handle cpu not the gpu if you want to convert a \n",
    "## tensor(gpu) to numpy then you have to convert/move it to cpu first\n",
    "\n",
    "tensor1=torch.empty(3,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# how to push tensor1 to gpu\n",
    "if torch.cuda.is_available():\n",
    "    device=torch.device(\"cuda\")\n",
    "    print(device)\n",
    "    tensorG=tensor1.to(device)\n",
    "    print(tensorG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device('cuda')\n",
    "    x=torch.ones(2,3,device=device)\n",
    "    x.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to tell pytorch that the declared tensor need to calculate the gradient \n",
    "## during the optimization phase i have to mention that requires_grad=True\n",
    "x=torch.ones(3,2,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 3.], device='cuda:0', dtype=torch.float16, requires_grad=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([3,3],dtype=torch.float16,device=torch.device('cuda'),requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "# backpropagation \n",
    "# first we need to know chain rule\n",
    "# dz/dx= dz/dy*dy/dx\n",
    "# next conceps computational graph\n",
    "#  linear regression\n",
    "#  loss=()\n",
    "\n",
    " ## \n",
    "import torch\n",
    "x=torch.tensor(1.0)\n",
    "y=torch.tensor(2.0)\n",
    "w=torch.tensor(1.0,requires_grad=True)\n",
    "y_hat=x*w\n",
    "loss=(y_hat-y)**2## forward pass \n",
    "loss.backward()## do the local gradient and backward pass \n",
    "print(w.grad)## update the weight according to the gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1:w=0.480,loss=9.00000000\n",
      "epoch3:w=1.028,loss=1.92578125\n",
      "epoch5:w=1.282,loss=0.41040039\n",
      "epoch7:w=1.399,loss=0.08813477\n",
      "epoch9:w=1.453,loss=0.01869202\n",
      "epoch11:w=1.478,loss=0.00390625\n",
      "epoch13:w=1.490,loss=0.00085831\n",
      "epoch15:w=1.495,loss=0.00018692\n",
      "epoch17:w=1.498,loss=0.00003433\n",
      "epoch19:w=1.499,loss=0.00001526\n",
      "epoch21:w=1.500,loss=0.00000381\n",
      "epoch23:w=1.500,loss=0.00000000\n",
      "epoch25:w=1.500,loss=0.00000000\n",
      "epoch27:w=1.500,loss=0.00000000\n",
      "epoch29:w=1.500,loss=0.00000000\n",
      "prediction after training:f(5)=7.498\n"
     ]
    }
   ],
   "source": [
    "## implementation of neural network in numpy \n",
    "import numpy as np\n",
    "# f=w*x\n",
    "# x=np.array([1,2,3,4],dtype=np.float16)\n",
    "# y=np.array([2,4,6,8],dtype=np.float16)\n",
    "x=np.array([2,2,2,2],dtype=np.float16)\n",
    "y=np.array([3,3,3,3],dtype=np.float16)\n",
    "w=0\n",
    "\n",
    "# MSE=1/N(w*x-y)**2\n",
    "# dMSE/dw=1/N(w*x)\n",
    "\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "def loss(y,y_predicted):\n",
    "    return ((y-y_predicted)**2).mean()\n",
    "\n",
    "def gradient(x,y,y_pred):\n",
    "    \n",
    "    return np.dot(2*x,(y_pred-y)).mean()\n",
    "\n",
    "learning_rate=.01\n",
    "n_iter=30\n",
    "\n",
    "for epoch in range(n_iter):\n",
    "    y_prediction = forward(x)\n",
    "    l= loss(y,y_prediction)\n",
    "    dw = gradient(x,y,y_prediction)\n",
    "    w -= learning_rate*dw\n",
    "    if epoch%2==0:\n",
    "        print(f'epoch{epoch+1}:w={w:.3f},loss={l:.8f}')\n",
    "\n",
    "print(f'prediction after training:f(5)={forward(5):.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "prediction before training f(5):0.000\n",
      "0\n",
      "epoch1:w=0.640,loss=43.25000000\n",
      "0.64\n",
      "epoch2:w=1.049,loss=34.84375000\n",
      "1.049375\n",
      "epoch3:w=1.312,loss=31.43750000\n",
      "1.3115625\n",
      "epoch4:w=1.479,loss=30.00000000\n",
      "1.4793749999999999\n",
      "epoch5:w=1.587,loss=29.43750000\n",
      "1.5868749999999998\n",
      "epoch6:w=1.656,loss=29.18750000\n",
      "1.6555468749999998\n",
      "epoch7:w=1.700,loss=29.09375000\n",
      "1.6996874999999998\n",
      "epoch8:w=1.728,loss=29.03125000\n",
      "1.727890625\n",
      "epoch9:w=1.746,loss=29.04687500\n",
      "1.7459375\n",
      "epoch10:w=1.757,loss=29.01562500\n",
      "1.7572656249999998\n",
      "epoch11:w=1.765,loss=29.01562500\n",
      "1.7648437499999998\n",
      "epoch12:w=1.770,loss=29.03125000\n",
      "1.769609375\n",
      "epoch13:w=1.773,loss=29.04687500\n",
      "1.7726562499999998\n",
      "epoch14:w=1.775,loss=29.01562500\n",
      "1.7746093749999998\n",
      "epoch15:w=1.776,loss=29.03125000\n",
      "1.7757812499999999\n",
      "epoch16:w=1.777,loss=29.03125000\n",
      "1.7765624999999998\n",
      "epoch17:w=1.777,loss=29.03125000\n",
      "1.7771875\n",
      "epoch18:w=1.777,loss=29.01562500\n",
      "1.7772656249999998\n",
      "epoch19:w=1.777,loss=29.01562500\n",
      "1.7773437499999998\n",
      "epoch20:w=1.777,loss=29.01562500\n",
      "1.7774218749999997\n",
      "prediction after training f(5):8.887\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x=np.array([2,2,3,1],dtype=np.float16)\n",
    "y=np.array([4,3,2,12],dtype=np.float16)\n",
    "w=0\n",
    "\n",
    "n_iter=20\n",
    "learning_rate=.01\n",
    "\n",
    "def forward(x):\n",
    "    # print('----------')\n",
    "    print(w)\n",
    "    return x*w\n",
    "def loss(y,y_pd):\n",
    "    return ((y_pd-y)**2).mean()\n",
    "def gradient(x,y,y_pd):\n",
    "    return np.dot(2*x,(y_pd-y)).mean()\n",
    "\n",
    "print(f\"prediction before training f(5):{forward(5):.3f}\")\n",
    "\n",
    "for epoch in range(n_iter):\n",
    "    y_pred=forward(x)\n",
    "\n",
    "    ls=loss(y,y_pred)\n",
    "\n",
    "    dw=gradient(x,y,y_pred)\n",
    "\n",
    "    w=w-learning_rate*dw\n",
    "  \n",
    "    print(f'epoch{epoch+1}:w={w:.3f},loss={ls:.8f}')\n",
    "\n",
    "print(f\"prediction after training f(5):{forward(5):.3f}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction of the modle before training f(6): 0.000\n",
      "----b------\n",
      "tensor(0.1600, requires_grad=True)\n",
      "-----a-----\n",
      "tensor(0.1600, requires_grad=True)\n",
      "----b------\n",
      "tensor(0.3056, requires_grad=True)\n",
      "-----a-----\n",
      "tensor(0.3056, requires_grad=True)\n",
      "epoch2:w=0.306,loss=40.80520\n",
      "----b------\n",
      "tensor(0.4381, requires_grad=True)\n",
      "-----a-----\n",
      "tensor(0.4381, requires_grad=True)\n",
      "epoch3:w=0.438,loss=38.78066\n",
      "----b------\n",
      "tensor(0.5587, requires_grad=True)\n",
      "-----a-----\n",
      "tensor(0.5587, requires_grad=True)\n",
      "epoch4:w=0.559,loss=37.10414\n",
      "----b------\n",
      "tensor(0.6684, requires_grad=True)\n",
      "-----a-----\n",
      "tensor(0.6684, requires_grad=True)\n",
      "epoch5:w=0.668,loss=35.71582\n",
      "----b------\n",
      "tensor(0.7682, requires_grad=True)\n",
      "-----a-----\n",
      "tensor(0.7682, requires_grad=True)\n",
      "epoch6:w=0.768,loss=34.56614\n",
      "----b------\n",
      "tensor(0.8591, requires_grad=True)\n",
      "-----a-----\n",
      "tensor(0.8591, requires_grad=True)\n",
      "epoch7:w=0.859,loss=33.61410\n",
      "----b------\n",
      "tensor(0.9418, requires_grad=True)\n",
      "-----a-----\n",
      "tensor(0.9418, requires_grad=True)\n",
      "epoch8:w=0.942,loss=32.82571\n",
      "----b------\n",
      "tensor(1.0170, requires_grad=True)\n",
      "-----a-----\n",
      "tensor(1.0170, requires_grad=True)\n",
      "epoch9:w=1.017,loss=32.17284\n",
      "----b------\n",
      "tensor(1.0855, requires_grad=True)\n",
      "-----a-----\n",
      "tensor(1.0855, requires_grad=True)\n",
      "epoch10:w=1.085,loss=31.63221\n",
      "prediction of the modle after training f(6): 5.427\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "x= torch.tensor([2,2,3,1],dtype=torch.float32)\n",
    "y=torch.tensor([4,3,2,12],dtype=torch.float32)\n",
    "w=torch.tensor(0,dtype=torch.float32,requires_grad=True)\n",
    "\n",
    "def forward(x):\n",
    "   return(x*w)\n",
    "def loss(y,ypd):\n",
    "   return((ypd-y)**2).mean()\n",
    "\n",
    "print(f\"prediction of the modle before training f(6): {forward(5).item():.3f}\")\n",
    "\n",
    "learning_rate=0.01\n",
    "n_iter=10\n",
    "\n",
    "for epoch in range(n_iter):\n",
    "   y_prediction=forward(x)\n",
    "   l=loss(y,y_prediction)\n",
    "   ################################ we have to do backpropagartion on loss function ################################################\n",
    "   l.backward()\n",
    "   with torch.no_grad():\n",
    "      w-=learning_rate*w.grad ### negative of w \n",
    "   print('----b------')\n",
    "   print(w)\n",
    "   w.grad.zero_()\n",
    "   print('-----a-----')\n",
    "   print(w)\n",
    "   if epoch % 20:\n",
    "      print(f'epoch{epoch+1}:w={w:.3f},loss={l:.5f}')\n",
    "\n",
    "print(f\"prediction of the modle after training f(6): {forward(5).item():.3f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature:1,Number of sample:4\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([[1],[2],[3],[4]],dtype=torch.float32)\n",
    "y=torch.tensor([[2],[4],[6],[8]],dtype=torch.float32)\n",
    "\n",
    "number_sample,number_feature=x.shape\n",
    "print(f'Number of feature:{number_feature},Number of sample:{number_sample}')\n",
    "input_size=number_feature\n",
    "output_size=number_feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature:1,Number of sample:4\n",
      "prediction of the modle before training f(6): 12.093\n",
      "epoch2:w=0.117,loss=39.85429\n",
      "epoch3:w=0.405,loss=27.66539\n",
      "epoch4:w=0.645,loss=19.20770\n",
      "epoch5:w=0.845,loss=13.33902\n",
      "epoch6:w=1.012,loss=9.26680\n",
      "epoch7:w=1.150,loss=6.44111\n",
      "epoch8:w=1.266,loss=4.48035\n",
      "epoch9:w=1.363,loss=3.11976\n",
      "epoch10:w=1.443,loss=2.17561\n",
      "epoch11:w=1.510,loss=1.52041\n",
      "epoch12:w=1.566,loss=1.06573\n",
      "epoch13:w=1.613,loss=0.75016\n",
      "epoch14:w=1.652,loss=0.53114\n",
      "epoch15:w=1.685,loss=0.37910\n",
      "epoch16:w=1.712,loss=0.27353\n",
      "epoch17:w=1.735,loss=0.20023\n",
      "epoch18:w=1.754,loss=0.14930\n",
      "epoch19:w=1.770,loss=0.11389\n",
      "epoch20:w=1.783,loss=0.08927\n",
      "epoch22:w=1.804,loss=0.06016\n",
      "epoch23:w=1.811,loss=0.05180\n",
      "epoch24:w=1.818,loss=0.04594\n",
      "epoch25:w=1.824,loss=0.04181\n",
      "epoch26:w=1.828,loss=0.03889\n",
      "epoch27:w=1.833,loss=0.03680\n",
      "epoch28:w=1.836,loss=0.03530\n",
      "epoch29:w=1.839,loss=0.03419\n",
      "epoch30:w=1.841,loss=0.03337\n",
      "epoch31:w=1.844,loss=0.03274\n",
      "epoch32:w=1.845,loss=0.03225\n",
      "epoch33:w=1.847,loss=0.03185\n",
      "epoch34:w=1.849,loss=0.03151\n",
      "epoch35:w=1.850,loss=0.03122\n",
      "epoch36:w=1.851,loss=0.03097\n",
      "epoch37:w=1.852,loss=0.03074\n",
      "epoch38:w=1.853,loss=0.03052\n",
      "epoch39:w=1.854,loss=0.03031\n",
      "epoch40:w=1.854,loss=0.03012\n",
      "epoch42:w=1.856,loss=0.02974\n",
      "epoch43:w=1.856,loss=0.02955\n",
      "epoch44:w=1.857,loss=0.02937\n",
      "epoch45:w=1.858,loss=0.02920\n",
      "epoch46:w=1.858,loss=0.02902\n",
      "epoch47:w=1.859,loss=0.02885\n",
      "epoch48:w=1.859,loss=0.02867\n",
      "epoch49:w=1.860,loss=0.02850\n",
      "epoch50:w=1.860,loss=0.02833\n",
      "epoch51:w=1.861,loss=0.02816\n",
      "epoch52:w=1.861,loss=0.02799\n",
      "epoch53:w=1.861,loss=0.02782\n",
      "epoch54:w=1.862,loss=0.02766\n",
      "epoch55:w=1.862,loss=0.02749\n",
      "epoch56:w=1.863,loss=0.02733\n",
      "epoch57:w=1.863,loss=0.02716\n",
      "epoch58:w=1.864,loss=0.02700\n",
      "epoch59:w=1.864,loss=0.02684\n",
      "epoch60:w=1.864,loss=0.02668\n",
      "epoch62:w=1.865,loss=0.02636\n",
      "epoch63:w=1.866,loss=0.02620\n",
      "epoch64:w=1.866,loss=0.02605\n",
      "epoch65:w=1.866,loss=0.02589\n",
      "epoch66:w=1.867,loss=0.02574\n",
      "epoch67:w=1.867,loss=0.02558\n",
      "epoch68:w=1.868,loss=0.02543\n",
      "epoch69:w=1.868,loss=0.02528\n",
      "epoch70:w=1.868,loss=0.02513\n",
      "epoch71:w=1.869,loss=0.02498\n",
      "epoch72:w=1.869,loss=0.02483\n",
      "epoch73:w=1.870,loss=0.02468\n",
      "epoch74:w=1.870,loss=0.02453\n",
      "epoch75:w=1.870,loss=0.02438\n",
      "epoch76:w=1.871,loss=0.02424\n",
      "epoch77:w=1.871,loss=0.02409\n",
      "epoch78:w=1.872,loss=0.02395\n",
      "epoch79:w=1.872,loss=0.02381\n",
      "epoch80:w=1.872,loss=0.02366\n",
      "epoch82:w=1.873,loss=0.02338\n",
      "epoch83:w=1.873,loss=0.02324\n",
      "epoch84:w=1.874,loss=0.02310\n",
      "epoch85:w=1.874,loss=0.02296\n",
      "epoch86:w=1.875,loss=0.02283\n",
      "epoch87:w=1.875,loss=0.02269\n",
      "epoch88:w=1.875,loss=0.02256\n",
      "epoch89:w=1.876,loss=0.02242\n",
      "epoch90:w=1.876,loss=0.02229\n",
      "epoch91:w=1.876,loss=0.02215\n",
      "epoch92:w=1.877,loss=0.02202\n",
      "epoch93:w=1.877,loss=0.02189\n",
      "epoch94:w=1.878,loss=0.02176\n",
      "epoch95:w=1.878,loss=0.02163\n",
      "epoch96:w=1.878,loss=0.02150\n",
      "epoch97:w=1.879,loss=0.02137\n",
      "epoch98:w=1.879,loss=0.02124\n",
      "epoch99:w=1.879,loss=0.02112\n",
      "epoch100:w=1.880,loss=0.02099\n",
      "prediction of the modle after training f(6): 9.399\n"
     ]
    }
   ],
   "source": [
    "### Design model(input size, output size, forward pass)\n",
    "### construct loss and opmtimizer \n",
    "### training loop\n",
    "#### -forward pass:make prediction\n",
    "#### -backward pass:compute gradient\n",
    "#### -update weight\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x=torch.tensor([[1],[2],[3],[4]],dtype=torch.float32)\n",
    "y=torch.tensor([[2],[4],[6],[8]],dtype=torch.float32)\n",
    "\n",
    "number_sample,number_feature=x.shape\n",
    "print(f'Number of feature:{number_feature},Number of sample:{number_sample}')\n",
    "input_size=number_feature\n",
    "output_size=number_feature\n",
    "\n",
    "model=nn.Linear(input_size,output_size)\n",
    "# w=torch.tensor(0.0,dtype=torch.float32,requires_grad=True)\n",
    "\n",
    "# def forward(x):\n",
    "#    return(x*w)\n",
    "loss=nn.MSELoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "# def loss(y,ypd):\n",
    "#    return((ypd-y)**2).mean()\n",
    "\n",
    "print(f\"prediction of the modle before training f(6): {forward(5).item():.3f}\")\n",
    "\n",
    "learning_rate=0.01\n",
    "n_iter=100\n",
    "\n",
    "for epoch in range(n_iter):\n",
    "   y_prediction=model(x)\n",
    "\n",
    "   l= loss(y,y_prediction)\n",
    "   # l=loss(y,y_prediction)\n",
    "   l.backward()\n",
    "   optimizer.step()\n",
    "   # with torch.no_grad():\n",
    "   #    w-=learning_rate*w.grad ### negative of w \n",
    "   optimizer.zero_grad()\n",
    "   # w.grad.zero_()\n",
    "\n",
    "   if epoch % 20:\n",
    "      w,b=model.parameters()\n",
    "      print(f'epoch{epoch+1}:w={w[0][0]:.3f},loss={l:.5f}')\n",
    "\n",
    "print(f\"prediction of the modle after training f(6): {forward(5).item():.3f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature:1,Number of sample:4\n",
      "prediction of the modle before training f(6): 8.542\n",
      "epoch2:w=0.863,loss=8.87734\n",
      "epoch3:w=0.998,loss=6.20770\n",
      "epoch4:w=1.111,loss=4.35500\n",
      "epoch5:w=1.206,loss=3.06917\n",
      "epoch6:w=1.285,loss=2.17667\n",
      "epoch7:w=1.350,loss=1.55711\n",
      "epoch8:w=1.405,loss=1.12693\n",
      "epoch9:w=1.451,loss=0.82816\n",
      "epoch10:w=1.490,loss=0.62057\n",
      "epoch11:w=1.522,loss=0.47625\n",
      "epoch12:w=1.549,loss=0.37584\n",
      "epoch13:w=1.571,loss=0.30590\n",
      "epoch14:w=1.590,loss=0.25710\n",
      "epoch15:w=1.606,loss=0.22297\n",
      "epoch16:w=1.620,loss=0.19902\n",
      "epoch17:w=1.631,loss=0.18213\n",
      "epoch18:w=1.641,loss=0.17016\n",
      "epoch19:w=1.649,loss=0.16158\n",
      "epoch20:w=1.656,loss=0.15537\n",
      "epoch22:w=1.667,loss=0.14738\n",
      "epoch23:w=1.671,loss=0.14475\n",
      "epoch24:w=1.675,loss=0.14267\n",
      "epoch25:w=1.679,loss=0.14097\n",
      "epoch26:w=1.682,loss=0.13954\n",
      "epoch27:w=1.684,loss=0.13830\n",
      "epoch28:w=1.686,loss=0.13719\n",
      "epoch29:w=1.689,loss=0.13618\n",
      "epoch30:w=1.690,loss=0.13523\n",
      "epoch31:w=1.692,loss=0.13433\n",
      "epoch32:w=1.694,loss=0.13346\n",
      "epoch33:w=1.695,loss=0.13261\n",
      "epoch34:w=1.697,loss=0.13179\n",
      "epoch35:w=1.698,loss=0.13098\n",
      "epoch36:w=1.699,loss=0.13018\n",
      "epoch37:w=1.700,loss=0.12939\n",
      "epoch38:w=1.701,loss=0.12861\n",
      "epoch39:w=1.702,loss=0.12784\n",
      "epoch40:w=1.703,loss=0.12707\n",
      "epoch42:w=1.705,loss=0.12555\n",
      "epoch43:w=1.706,loss=0.12480\n",
      "epoch44:w=1.707,loss=0.12405\n",
      "epoch45:w=1.708,loss=0.12331\n",
      "epoch46:w=1.709,loss=0.12257\n",
      "epoch47:w=1.710,loss=0.12184\n",
      "epoch48:w=1.711,loss=0.12111\n",
      "epoch49:w=1.712,loss=0.12039\n",
      "epoch50:w=1.713,loss=0.11967\n",
      "epoch51:w=1.714,loss=0.11895\n",
      "epoch52:w=1.715,loss=0.11824\n",
      "epoch53:w=1.715,loss=0.11753\n",
      "epoch54:w=1.716,loss=0.11683\n",
      "epoch55:w=1.717,loss=0.11613\n",
      "epoch56:w=1.718,loss=0.11544\n",
      "epoch57:w=1.719,loss=0.11475\n",
      "epoch58:w=1.720,loss=0.11406\n",
      "epoch59:w=1.721,loss=0.11338\n",
      "epoch60:w=1.721,loss=0.11270\n",
      "epoch62:w=1.723,loss=0.11136\n",
      "epoch63:w=1.724,loss=0.11069\n",
      "epoch64:w=1.725,loss=0.11003\n",
      "epoch65:w=1.726,loss=0.10937\n",
      "epoch66:w=1.726,loss=0.10872\n",
      "epoch67:w=1.727,loss=0.10807\n",
      "epoch68:w=1.728,loss=0.10742\n",
      "epoch69:w=1.729,loss=0.10678\n",
      "epoch70:w=1.730,loss=0.10614\n",
      "epoch71:w=1.730,loss=0.10551\n",
      "epoch72:w=1.731,loss=0.10488\n",
      "epoch73:w=1.732,loss=0.10425\n",
      "epoch74:w=1.733,loss=0.10363\n",
      "epoch75:w=1.734,loss=0.10301\n",
      "epoch76:w=1.734,loss=0.10239\n",
      "epoch77:w=1.735,loss=0.10178\n",
      "epoch78:w=1.736,loss=0.10117\n",
      "epoch79:w=1.737,loss=0.10057\n",
      "epoch80:w=1.738,loss=0.09996\n",
      "epoch82:w=1.739,loss=0.09877\n",
      "epoch83:w=1.740,loss=0.09818\n",
      "epoch84:w=1.741,loss=0.09759\n",
      "epoch85:w=1.742,loss=0.09701\n",
      "epoch86:w=1.742,loss=0.09643\n",
      "epoch87:w=1.743,loss=0.09585\n",
      "epoch88:w=1.744,loss=0.09528\n",
      "epoch89:w=1.745,loss=0.09471\n",
      "epoch90:w=1.745,loss=0.09415\n",
      "epoch91:w=1.746,loss=0.09358\n",
      "epoch92:w=1.747,loss=0.09302\n",
      "epoch93:w=1.748,loss=0.09247\n",
      "epoch94:w=1.748,loss=0.09191\n",
      "epoch95:w=1.749,loss=0.09136\n",
      "epoch96:w=1.750,loss=0.09082\n",
      "epoch97:w=1.751,loss=0.09028\n",
      "epoch98:w=1.751,loss=0.08974\n",
      "epoch99:w=1.752,loss=0.08920\n",
      "epoch100:w=1.753,loss=0.08867\n",
      "prediction of the model after training f(5): 8.765\n"
     ]
    }
   ],
   "source": [
    "### Design model(input size, output size, forward pass)\n",
    "### construct loss and opmtimizer \n",
    "### training loop\n",
    "#### -forward pass:make prediction\n",
    "#### -backward pass:compute gradient\n",
    "#### -update weight\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x=torch.tensor([[1],[2],[3],[4]],dtype=torch.float32)\n",
    "y=torch.tensor([[2],[4],[6],[8]],dtype=torch.float32)\n",
    "\n",
    "number_sample,number_feature=x.shape\n",
    "print(f'Number of feature:{number_feature},Number of sample:{number_sample}')\n",
    "input_size=number_feature\n",
    "output_size=number_feature\n",
    "\n",
    "model=nn.Linear(input_size,output_size)\n",
    "\n",
    "class Linear_regression(nn.Module):\n",
    "   def __init__(self,input_dim,output_dim):\n",
    "      super(Linear_regression,self).__init__()\n",
    "      self.ln=nn.Linear(input_dim,output_dim)\n",
    "   def forward(self,x):\n",
    "      return self.ln(x)\n",
    "\n",
    "model=Linear_regression(input_size,output_size)\n",
    "# w=torch.tensor(0.0,dtype=torch.float32,requires_grad=True)\n",
    "\n",
    "# def forward(x):\n",
    "#    return(x*w)\n",
    "loss=nn.MSELoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "# def loss(y,ypd):\n",
    "#    return((ypd-y)**2).mean()\n",
    "\n",
    "print(f\"prediction of the modle before training f(6): {forward(5).item():.3f}\")\n",
    "\n",
    "learning_rate=0.01\n",
    "n_iter=100\n",
    "\n",
    "for epoch in range(n_iter):\n",
    "   y_prediction=model(x)\n",
    "\n",
    "   l= loss(y,y_prediction)\n",
    "   # l=loss(y,y_prediction)\n",
    "   l.backward()\n",
    "   optimizer.step()\n",
    "   # with torch.no_grad():\n",
    "   #    w-=learning_rate*w.grad ### negative of w \n",
    "   optimizer.zero_grad()\n",
    "   # w.grad.zero_()\n",
    "\n",
    "   if epoch % 20:\n",
    "      w,b=model.parameters()\n",
    "      print(f'epoch{epoch+1}:w={w[0][0]:.3f},loss={l:.5f}')\n",
    "\n",
    "print(f\"prediction of the model after training f(5): {forward(5).item():.3f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, weight: 0.9567664861679077, loss: 5682.6943359375\n",
      "epoch: 101, weight: 63.998443603515625, loss: 407.8380432128906\n",
      "epoch: 201, weight: 73.30953979492188, loss: 292.774169921875\n",
      "epoch: 301, weight: 74.69132232666016, loss: 290.2186279296875\n",
      "epoch: 401, weight: 74.89725494384766, loss: 290.1610412597656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x211b659c790>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## try to incorporate all our learn concepts\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "## prepare dataset\n",
    "x_numpy,y_numpy=datasets.make_regression(n_samples=100,n_features=1,noise=20,random_state=4)\n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "X=torch.from_numpy(x_numpy.astype(np.float32))\n",
    "y=torch.from_numpy(y_numpy.astype(np.float32))\n",
    "\n",
    "y = y.view(y.shape[0], 1)\n",
    "\n",
    "\n",
    "## Define model \n",
    "class regression(nn.Module):\n",
    "   def __init__(self,input_size,output_size):\n",
    "      super(regression,self).__init__()\n",
    "      self.ln=nn.Linear(input_size,output_size)\n",
    "\n",
    "   def forward(self,x):\n",
    "      return self.ln(x)\n",
    "sample,feature=X.shape\n",
    "# print(feature)\n",
    "input_size=feature\n",
    "output_size=feature\n",
    "model=regression(input_size,output_size)\n",
    "# print(model)\n",
    "## Train\n",
    "\n",
    "## Model parameters\n",
    "learning_rate=0.01\n",
    "n_iter=500\n",
    "ls=nn.MSELoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "# X_pred_b=model(X).detach().numpy()\n",
    "\n",
    "## train\n",
    "for epoch in range(n_iter):\n",
    "   y_pred=model(X)\n",
    "   # print(y.type)\n",
    "   loss=ls(y,y_pred)\n",
    "   # print(type(loss))\n",
    "   loss.backward()\n",
    "   optimizer.step()\n",
    "   optimizer.zero_grad()\n",
    "   if epoch%100==0:\n",
    "      w,b=model.parameters()\n",
    "      print(f\"epoch: {epoch+1}, weight: {w[0][0]}, loss: {loss}\")\n",
    "y_pd=model(X).detach().numpy()\n",
    "\n",
    "pl.plot(x_numpy,y_pd,'r')\n",
    "pl.scatter(X_numpy,y_numpy)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, loss: 0.6791,accuracy: 0.5677\n",
      "epoch: 6, loss: 0.6712,accuracy: 0.5797\n",
      "epoch: 9, loss: 0.6636,accuracy: 0.5931\n",
      "epoch: 12, loss: 0.6562,accuracy: 0.6075\n",
      "epoch: 15, loss: 0.6491,accuracy: 0.6192\n",
      "epoch: 18, loss: 0.6422,accuracy: 0.6299\n",
      "epoch: 21, loss: 0.6355,accuracy: 0.6459\n",
      "epoch: 24, loss: 0.6290,accuracy: 0.6560\n",
      "epoch: 27, loss: 0.6227,accuracy: 0.6677\n",
      "epoch: 30, loss: 0.6166,accuracy: 0.6744\n",
      "epoch: 33, loss: 0.6107,accuracy: 0.6827\n",
      "epoch: 36, loss: 0.6049,accuracy: 0.6928\n",
      "epoch: 39, loss: 0.5994,accuracy: 0.7000\n",
      "epoch: 42, loss: 0.5940,accuracy: 0.7045\n",
      "epoch: 45, loss: 0.5887,accuracy: 0.7112\n",
      "epoch: 48, loss: 0.5836,accuracy: 0.7157\n",
      "epoch: 51, loss: 0.5787,accuracy: 0.7232\n",
      "epoch: 54, loss: 0.5739,accuracy: 0.7299\n",
      "epoch: 57, loss: 0.5692,accuracy: 0.7363\n",
      "epoch: 60, loss: 0.5647,accuracy: 0.7408\n",
      "epoch: 63, loss: 0.5603,accuracy: 0.7461\n",
      "epoch: 66, loss: 0.5560,accuracy: 0.7512\n",
      "epoch: 69, loss: 0.5518,accuracy: 0.7549\n",
      "epoch: 72, loss: 0.5478,accuracy: 0.7584\n",
      "epoch: 75, loss: 0.5438,accuracy: 0.7624\n",
      "epoch: 78, loss: 0.5400,accuracy: 0.7672\n",
      "epoch: 81, loss: 0.5363,accuracy: 0.7725\n",
      "epoch: 84, loss: 0.5326,accuracy: 0.7771\n",
      "epoch: 87, loss: 0.5291,accuracy: 0.7803\n",
      "epoch: 90, loss: 0.5256,accuracy: 0.7835\n",
      "epoch: 93, loss: 0.5222,accuracy: 0.7877\n",
      "epoch: 96, loss: 0.5190,accuracy: 0.7904\n",
      "epoch: 99, loss: 0.5158,accuracy: 0.7939\n",
      "accuracy: 0.8016\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 0) Prepare data\n",
    "X_numpy, y_numpy = datasets.make_classification(n_samples=5000,n_classes=2, random_state=4)\n",
    "import matplotlib.pyplot as pl\n",
    "# pl.scatter(X_numpy,y_numpy)\n",
    "# cast to float Tensor\n",
    "x_train,  x_test, y_train,y_test =train_test_split(X_numpy, y_numpy)\n",
    "\n",
    "x_train = torch.from_numpy(x_train.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "x_test = torch.from_numpy(x_test.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test= y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "n_samples, n_features = x_train.shape\n",
    "\n",
    "# 1) Model\n",
    "# Linear model f = wx + b\n",
    "input_size = n_features\n",
    "output_class = 1\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size,output_class):\n",
    "        super(Model, self).__init__()\n",
    "        self.ln=nn.Linear(input_size,output_class)\n",
    "    def forward(self,x):\n",
    "        p=torch.sigmoid(self.ln(x))\n",
    "        return(p)\n",
    "        \n",
    "model = Model(input_size, output_class)\n",
    "# 2) Loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# 3) Training loop\n",
    "num_epochs = 100\n",
    "def accuracy_P(y,yp):\n",
    "    with torch.no_grad():\n",
    "        y_predP=yp.round()\n",
    "        \n",
    "        acc=y_predP.eq(y).sum()/float(y.shape[0])\n",
    "\n",
    "        \n",
    "    return acc.item()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass and loss\n",
    "    y_predicted = model(x_train)\n",
    "    # print(y.type)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        acc=accuracy_P(y_train,y_predicted)\n",
    "        # print(acc)\n",
    "\n",
    "    # print(type(loss))\n",
    "    \n",
    "    # Backward pass and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero grad before new step\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1) % 3 == 0:\n",
    "        print(f'epoch: {epoch+1}, loss: {loss.item():.4f},accuracy: {acc:.4f}')\n",
    "\n",
    "# prediction\n",
    "\n",
    "# with torch.no_grad():\n",
    "y_pred=model(x_test)\n",
    "y_predP=y_pred.round()\n",
    "acc=y_predP.eq(y_test).sum()/float(y_test.shape[0])\n",
    "print(f'accuracy: {acc.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 0) Prepare data\n",
    "X_numpy, y_numpy = datasets.make_classification(n_samples=100,n_classes=2, random_state=4)\n",
    "import matplotlib.pyplot as pl\n",
    "# pl.scatter(X_numpy,y_numpy)\n",
    "# cast to float Tensor\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "y = y.view(y.shape[0], 1)\n",
    "y.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1703e62da64a32a11f00f57ac9ce29889bc8366799c7e287ccba831cb7c5c4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
